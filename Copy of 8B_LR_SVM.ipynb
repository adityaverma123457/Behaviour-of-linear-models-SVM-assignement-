{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Copy of 8B_LR_SVM.ipynb","provenance":[{"file_id":"1BUpnDVbidPKun9dzAAv14YowGCnu8puD","timestamp":1615635550222}],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"8ArWK463kbhL","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1621828918159,"user_tz":-330,"elapsed":58,"user":{"displayName":"Aditya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOGTA563GQR2_tXeSel3x_dRXqLgHMGN_2CcOaLA=s64","userId":"09522989787960602189"}},"outputId":"d08e5357-6612-4b0f-8f04-0896c20e1f21"},"source":["import numpy as np\n","import pandas as pd\n","import plotly\n","import plotly.figure_factory as ff\n","import plotly.graph_objs as go\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n","init_notebook_mode(connected=True)\n","import time\n","\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["        <script type=\"text/javascript\">\n","        window.PlotlyConfig = {MathJaxConfig: 'local'};\n","        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n","        if (typeof require !== 'undefined') {\n","        require.undef(\"plotly\");\n","        requirejs.config({\n","            paths: {\n","                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n","            }\n","        });\n","        require(['plotly'], function(Plotly) {\n","            window._Plotly = Plotly;\n","        });\n","        }\n","        </script>\n","        "]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6I4zMYeoke5l","executionInfo":{"status":"ok","timestamp":1621828920147,"user_tz":-330,"elapsed":708,"user":{"displayName":"Aditya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOGTA563GQR2_tXeSel3x_dRXqLgHMGN_2CcOaLA=s64","userId":"09522989787960602189"}},"outputId":"007091c1-1cfd-4816-ab0c-8c38f23b3447"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5mldzJdakbhS"},"source":["data = pd.read_csv('/content/drive/MyDrive/APPLIED AI ASSIGNMENTS ipynb/Behaviour of linear models/Copy of task_b.csv')\n","data=data.iloc[:,1:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"rsCrC2wckbhV","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1621828921633,"user_tz":-330,"elapsed":36,"user":{"displayName":"Aditya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOGTA563GQR2_tXeSel3x_dRXqLgHMGN_2CcOaLA=s64","userId":"09522989787960602189"}},"outputId":"4956622a-d1cd-4eab-c22a-749cb8d22220"},"source":["data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>f1</th>\n","      <th>f2</th>\n","      <th>f3</th>\n","      <th>y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-195.871045</td>\n","      <td>-14843.084171</td>\n","      <td>5.532140</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-1217.183964</td>\n","      <td>-4068.124621</td>\n","      <td>4.416082</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>9.138451</td>\n","      <td>4413.412028</td>\n","      <td>0.425317</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>363.824242</td>\n","      <td>15474.760647</td>\n","      <td>1.094119</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-768.812047</td>\n","      <td>-7963.932192</td>\n","      <td>1.870536</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            f1            f2        f3    y\n","0  -195.871045 -14843.084171  5.532140  1.0\n","1 -1217.183964  -4068.124621  4.416082  1.0\n","2     9.138451   4413.412028  0.425317  0.0\n","3   363.824242  15474.760647  1.094119  0.0\n","4  -768.812047  -7963.932192  1.870536  0.0"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"JA0gxk35NHgw","executionInfo":{"status":"ok","timestamp":1621828922220,"user_tz":-330,"elapsed":26,"user":{"displayName":"Aditya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOGTA563GQR2_tXeSel3x_dRXqLgHMGN_2CcOaLA=s64","userId":"09522989787960602189"}},"outputId":"bd5afbda-8400-48dd-add6-c2ba1e3d6ab6"},"source":["data.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>f1</th>\n","      <th>f2</th>\n","      <th>f3</th>\n","      <th>y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>200.000000</td>\n","      <td>200.000000</td>\n","      <td>200.000000</td>\n","      <td>200.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>10.180031</td>\n","      <td>1299.986739</td>\n","      <td>5.001840</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>488.195035</td>\n","      <td>10403.417325</td>\n","      <td>2.926662</td>\n","      <td>0.501255</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>-1662.579110</td>\n","      <td>-29605.563847</td>\n","      <td>0.076763</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>-303.220980</td>\n","      <td>-5626.637315</td>\n","      <td>2.508042</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>4.684317</td>\n","      <td>2611.405803</td>\n","      <td>5.029256</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>312.239850</td>\n","      <td>8075.864754</td>\n","      <td>7.436617</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1130.609573</td>\n","      <td>24131.360720</td>\n","      <td>9.933769</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                f1            f2          f3           y\n","count   200.000000    200.000000  200.000000  200.000000\n","mean     10.180031   1299.986739    5.001840    0.500000\n","std     488.195035  10403.417325    2.926662    0.501255\n","min   -1662.579110 -29605.563847    0.076763    0.000000\n","25%    -303.220980  -5626.637315    2.508042    0.000000\n","50%       4.684317   2611.405803    5.029256    0.500000\n","75%     312.239850   8075.864754    7.436617    1.000000\n","max    1130.609573  24131.360720    9.933769    1.000000"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"FI18joJ_kbhZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621828923025,"user_tz":-330,"elapsed":20,"user":{"displayName":"Aditya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOGTA563GQR2_tXeSel3x_dRXqLgHMGN_2CcOaLA=s64","userId":"09522989787960602189"}},"outputId":"fc5bd71b-160f-4678-dd74-0daf4f365c26"},"source":["data.corr()['y']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["f1    0.067172\n","f2   -0.017944\n","f3    0.839060\n","y     1.000000\n","Name: y, dtype: float64"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"yQIbNaHskbhe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621828926503,"user_tz":-330,"elapsed":14,"user":{"displayName":"Aditya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOGTA563GQR2_tXeSel3x_dRXqLgHMGN_2CcOaLA=s64","userId":"09522989787960602189"}},"outputId":"6522a6d3-83d7-477d-fbbb-ed1546272814"},"source":["X=data[['f1','f2','f3']].values\n","Y=data['y'].values\n","print(X.shape)\n","print(Y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(200, 3)\n","(200,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aUxp9-qEkbhh"},"source":["# What if our features are with different variance \n","\n","<pre>\n","* <b>As part of this task you will observe how linear models work in case of data having feautres with different variance</b>\n","* <b>from the output of the above cells you can observe that var(F2)>>var(F1)>>Var(F3)</b>\n","\n","> <b>Task1</b>:\n","    1. Apply Logistic regression(SGDClassifier with logloss) on 'data' and check the feature importance\n","    2. Apply SVM(SGDClassifier with hinge) on 'data' and check the feature importance\n","\n","> <b>Task2</b>:\n","    1. Apply Logistic regression(SGDClassifier with logloss) on 'data' after standardization \n","       i.e standardization(data, column wise): (column-mean(column))/std(column) and check the feature importance\n","    2. Apply SVM(SGDClassifier with hinge) on 'data' after standardization \n","       i.e standardization(data, column wise): (column-mean(column))/std(column) and check the feature importance\n","\n","</pre>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3oRzYo32neoA","executionInfo":{"status":"ok","timestamp":1621828980275,"user_tz":-330,"elapsed":370,"user":{"displayName":"Aditya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOGTA563GQR2_tXeSel3x_dRXqLgHMGN_2CcOaLA=s64","userId":"09522989787960602189"}},"outputId":"fa609d00-9002-4a12-b2d6-2b34159ef0f0"},"source":["from sklearn.linear_model import SGDClassifier\n","#################################################################\n","start = time.time()\n","classifier_log=SGDClassifier(loss='log' )\n","classifier_log.fit(X,Y)\n","importance=classifier_log.coef_[0]\n","#print(classifier_log.coef_)\n","#importance=zip(list(data.columns[:-1]),list(classifier.coef_[0]))\n","for i, feature in enumerate(data.columns[:-1]):\n","  print(\"\\nfeature:{} ,importance:{} after applying logistic regression\".format(feature,importance[i]))\n","print(\"--\"*20,\"\\nUSING SVC ON SAME DATA\")\n","#plt.\n","\n","classifier_hinge=SGDClassifier(loss='hinge')\n","classifier_hinge.fit(X,Y)\n","importance_hinge=classifier_hinge.coef_[0]\n","for i, feature in enumerate(data.columns[:-1]):\n","  print(\"\\nfeature:{} ,importance:{} after applying SVC\".format(feature,importance_hinge[i]))\n","end = time.time()\n","print(\"time elapsed:\",(end-start))\n","###########################################################\n","start = time.time()\n","\n","print(\"**\"*20,\"\\nApplying Logistic Regression after Standard Scaling\")\n","scaler=StandardScaler()\n","X_sc=scaler.fit_transform(X)\n","classifier_log.fit(X_sc,Y)\n","classifier_hinge.fit(X_sc,Y)\n","importance_log=classifier_log.coef_[0]\n","importance_hinge=classifier_hinge.coef_[0]\n","for i, feature in enumerate(data.columns[:-1]):\n","  print(\"\\nfeature:{} ,importance:{}\".format(feature,importance[i]))\n","\n","print(\"--\"*20,\"\\nUSING SVC ON SAME DATA\")  \n","for i, feature in enumerate(data.columns[:-1]):\n","  print(\"\\nfeature:{} ,importance:{}\".format(feature,importance_hinge[i]))\n","\n","end = time.time()\n","print(\"time elapsed after scaling:\",(end-start))\n","##############################################################"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","feature:f1 ,importance:6981.358515102275 after applying logistic regression\n","\n","feature:f2 ,importance:13828.787143784248 after applying logistic regression\n","\n","feature:f3 ,importance:10452.09348642178 after applying logistic regression\n","---------------------------------------- \n","USING SVC ON SAME DATA\n","\n","feature:f1 ,importance:9486.99202672657 after applying SVC\n","\n","feature:f2 ,importance:2804.7393130529776 after applying SVC\n","\n","feature:f3 ,importance:10480.658852517101 after applying SVC\n","time elapsed: 0.007037162780761719\n","**************************************** \n","Applying Logistic Regression after Standard Scaling\n","\n","feature:f1 ,importance:6981.358515102275\n","\n","feature:f2 ,importance:13828.787143784248\n","\n","feature:f3 ,importance:10452.09348642178\n","---------------------------------------- \n","USING SVC ON SAME DATA\n","\n","feature:f1 ,importance:0.6101648788393268\n","\n","feature:f2 ,importance:-1.9200484468140877\n","\n","feature:f3 ,importance:16.29490145508181\n","time elapsed after scaling: 0.008134841918945312\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I_a9L3PS6Awq","executionInfo":{"status":"ok","timestamp":1621828988008,"user_tz":-330,"elapsed":450,"user":{"displayName":"Aditya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOGTA563GQR2_tXeSel3x_dRXqLgHMGN_2CcOaLA=s64","userId":"09522989787960602189"}},"outputId":"4e940599-cf02-48b6-9fbb-e8132d26f25c"},"source":["print(\"**\"*20,\"\\nApplying Logistic Regression after MinMax Scaling\")\n","scaler=MinMaxScaler()\n","X_sc=scaler.fit_transform(X)\n","classifier_log.fit(X_sc,Y)\n","classifier_hinge.fit(X_sc,Y)\n","importance_log=classifier_log.coef_[0]\n","importance_hinge=classifier_hinge.coef_[0]\n","for i, feature in enumerate(data.columns[:-1]):\n","  print(\"\\nfeature:{} ,importance:{}\".format(feature,importance[i]))\n","\n","print(\"--\"*20,\"\\nUSING SVC ON SAME DATA\")  \n","for i, feature in enumerate(data.columns[:-1]):\n","  print(\"\\nfeature:{} ,importance:{}\".format(feature,importance_hinge[i]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["**************************************** \n","Applying Logistic Regression after MinMax Scaling\n","\n","feature:f1 ,importance:6981.358515102275\n","\n","feature:f2 ,importance:13828.787143784248\n","\n","feature:f3 ,importance:10452.09348642178\n","---------------------------------------- \n","USING SVC ON SAME DATA\n","\n","feature:f1 ,importance:-3.766443943629567\n","\n","feature:f2 ,importance:-2.15217261057111\n","\n","feature:f3 ,importance:52.46587229147704\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TbMnsrxakbhi"},"source":["<h3><font color='blue'> Make sure you write the observations for each task, why a particular feautre got more importance than others</font></h3>"]},{"cell_type":"code","metadata":{"id":"G-BR5Tu0_-yV"},"source":["CONCLUSION:\n","1) AS WE SEE IN THE HIGH LEVEL STATISTICS [DATA.DESCRIBE()] ORDER OF STANDARD DEVIATION WAS F2>>F1>>F3 AND MEAN ORDER WAS F2>>F1>>F3/\n","AFTER TRAINING THIS DATA ON SVM WITHOUT SCALING GAVE FEATURE IMPORTANCE IN ORDER OF F2>>F3>>F1 THIS MUST BE BECAUSE OF LARGER /\n","NUMERICAL RANGE OF F2 ,F2 HAS IMPACTED WEIGHTS OF DECISION BOUNDARY HEAVILY. BUT AFTER STANDARDISATION ALL THE FEATURES WERE BROUGHT IN SAME SCALE WHICH/\n","ON BEING RUN ON SVM GAVE FEATURE IMPORTANCE ORDER AS F3>>F1>>F2 AND THIS RESULT IS ROBUST BECAUSE IT IS GENERATED BY THE SCALED DATA WHICH IS FREE FROM/\n","NUMERICAL BIASES THAT HAD OCCURED DUE TO DIFFERENT NUMERICAL RANGES OF FEATURES AND  HAD CONFUSED THE MODEL DUE TO HIGH MAGNITUDE.APART FROM ABOVE MENTIONED BENEFIT/\n","SCALING ALSO REDUCES TIME COMPLEXITY DUE TO SIMPLER CALCULATIONS AFTER SCALING.\n","\n","2) HERE MAGNITUDE OF FEATURE IMPORTANCE SHOWS HOW INFORMATIVE THIS PARTICULAR FEATURE IS FOR PREDICTION AND SIGN REPRESENTS WHICH CLASS (+/-) THIS PARTICULAR/\n","FEATURE IS USEFUL FOR.\n","\n","3) IN CASE OF LogisticRegression SCALING IS NOT NECCESARY FOR PARTICULATLY BEACAUSE OF SQUASHING WHICH TURNS ALL THE DISTANCES FROM DECISION BOUNDARY IN RANGE OF (0 ,1)/\n","CONSEQUENTLY ALL THE DISTANCES REDUCE DOWN TO SAME SCALE BUT STILL SCALING COULD HELP IN FASTER CONVERGENCE (LOW TIME COMPLEXITY) DURING OPTIMISATION FOR THE SAME REASON IT IS RECOMMENDED TO /\n","DO FEATURE SCALING EVEN IN LOGISTIC REGRESSION.FOR THE ABOVE REASON FINAL ORDER OF FEATURE IMPORTANCE IS SAME NO MATTER YOU DO IT WITH SCALING OR WITHOUT.\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ILy_66tBW6Az"},"source":[""]}]}